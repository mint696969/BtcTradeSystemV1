# 2-6 機能分離リポ再設計・実装予定.md

## 26. 再始動プレイブック（確定 / 2025-10-12）

**目的**: プロジェクト再始動時に迷わず着手し、最初の 30 分で“文脈復元 → 健康チェック → 作業再開”まで到達する。

### 26.1 最初の 30 分チェックリスト（順番固定）

1. **環境宣言**（必須）

   - `env_manifest.yaml` を確認（branch/commit/mode/actor/site）
   - `scripts/run.ps1 -WhatIf` で起動前検証（依存/パス/権限）

2. **文脈復元**

   - `tools/make_context_bundle.ps1 -Load`（前回の `CTX-*.zip` を指定）
   - GPT へ `gpt_context_map.yaml` → `env_manifest.yaml` → `repo.contract.yaml` の順にアップロード

3. **健全性スモーク**（Collector 停止のまま）

   - `scripts/diag/diag_api.ps1`（IPv4 優先）
   - `ops/collector/health.ps1 -DryRun`（latest/raw 整合・閾値表示のみ）

4. **設定差分の確認**

   - `tools/diff_config.ps1`（defaults vs registry/ui/secrets）

5. **ハンドオーバー読込**

   - `handover.yaml` の `next_tasks` をダッシュボードに掲示（UI の ToDo エリア）

### 26.2 “最初のタスク”：引継ぎ資料の**自動作成**（固定）

- 再始動直後に必ず実行：`tools/make_context_bundle.ps1`（新規 `CTX-*.zip` 生成）
- 生成内容：第 25 章 25.1 の一式（`env_manifest.yaml`/`handover.yaml`/`gpt_context_map.yaml`/…）
- 監査：`audit.jsonl` に `handover.bundle.create` を記録（actor/site/session/zip_path/hash）

### 26.3 モードと安全網

- 再始動時の既定モードは **DEBUG（黄）**。Collector 起動前の確認が済んだら **PROD（緑）** に遷移。
- 失敗系が出たら **DIAG（赤）** に一時切替して根拠採取 → 速やかに緑へ戻す。

### 26.4 実行コマンド例（PowerShell）

```powershell
# 1) 起動前検証
scripts/run.ps1 -WhatIf

# 2) 前回文脈の読込（任意）
tools/make_context_bundle.ps1 -Load ./artifacts/context_bundle/CTX-2025...zip

# 3) スモーク
scripts/diag/diag_api.ps1
ops/collector/health.ps1 -DryRun

# 4) 引継ぎ資料の自動作成（必ず）
tools/make_context_bundle.ps1
```

### 27 ハンドオフ仕様（Handoff System Specification）

■ 概要

ハンドオフは、開発セッションの区切り・引き継ぎ・チャットまたぎを自動化するための軽量アーカイブ生成システムであり、
1 クリックで「再開可能な完全文脈パッケージ（Handoff_YYYYMMDD_HHMM.zip）」を生成する。

■ 目的

チャットをまたいでも開発状態を即再現できること。

リポジトリ構造・環境・作業状況を自動収集し、ノイズを排除すること。

GPT と人間の両方がシームレスに開発を継続できること。

■ 生成対象
種別 格納パス 内容
🗂 構成図 repo*structure.yaml ディレクトリ構造＋各ファイル先頭コメント 2 行。
⚙️ 環境定義 env_manifest.yaml OS / Python / Streamlit / 依存関係バージョン。
🧠 開発文脈 gpt_context_map.yaml 現在フェーズ・モード・作業中ファイル・未完タスク。
🪪 作業履歴 handover.md ライブ引継ぎログ（完了・残タスク・次回方針）。
🧰 スクリプト scripts/handoff/make_handoff.ps1 ハンドオフ生成スクリプト。
🔖 復元点 scripts/git/git_rp*\*.ps1 Git ロールバック関連スクリプト。
🧱 付随データ data_sample/, logs_sample/ データ／ログ構造のダミー。

※「開発ルール」と「構成思想（2-1〜2-6）」はプロジェクトファイル内で管理するため、ZIP からは除外。

■ 動作仕様

実行方法：

& .\scripts\handoff\make_handoff.ps1

またはルート直下の make_handoff.bat をクリック。
実行後、完了ログとともに ZIP を生成する。

内部処理フロー：

環境情報収集 → env_manifest.yaml

Git タグ・ブランチ抽出

ファイル構造走査 → repo_structure.yaml

handover.md／gpt_context_map.yaml 同梱

ZIP 化して docs/handoff/ に出力。

■ LiveHandoff 連携

Handoff ZIP 内の handover.md と gpt_context_map.yaml により、
次チャットでアップロードすると自動で開発フェーズ・モード・残タスクを復元。

GPT はこれらを解析し、「続きから開始します」と応答する。

■ 手動運用ルール
操作 コマンド 用途
日次ハンドオフ make_handoff.bat 毎日の作業終了時に実行。
復元ポイント作成 git_rp_make.ps1 大改修や節目の前に実行。
復元ポイント一覧 git_rp_list.ps1 既存 rp タグの確認。
復元 git_rp_restore.ps1 指定タグへロールバック。
■ 関連運用

ハンドオフ：日次引継ぎ（軽量）

復元ポイント：差分追跡（Git タグ）

フルバックアップ：節目や重大更新時（手動）
三層構成で安全性と再現性を両立する。

■ 今後の拡張

make_handoff.ps1 に自動 rp タグ発行を追加予定。

gpt_context_map.yaml に開発者ノート欄を追加。

LiveHandoff と CTX バンドル統合によるワンクリック完全引継ぎを最終目標とする。

### 28. 二台構成（収集 PC ＋メイン PC）と NAS 前提の収集・冗長化設計（確定ドラフト / 2025-10-14）

目的: 収集 PC を常時稼働の“リーダー”としつつ、メイン PC が一時的に収集を肩代わりできる構成にする。データは **NAS を一次ストレージ（連続保持の唯一の正）**とし、NAS メンテ中はローカルに退避・復旧後に 順序保証つきで同期する。UI や監査、既存の Health 判定に自然接続する。

### 28.1 トポロジ・前提

ノード
収集 PC（Leader 想定 / 24h 稼働）
メイン PC（Dashboard 操作主体 / 一時収集 Failover 役）
NAS（Primary ストレージ / 連続履歴の正本）
原則
単一アクティブ収集（同時二重収集を禁止）
NAS は常に連続データ（欠番なし）を保持
一時退避はローカル（各 PC のローカル）→ 復旧後に NAS へ同期 → ローカル削除
監査（logs/audit.jsonl）は重要イベントを必ず記録

### 28.2 ロールと状態（status.json 拡張）

data/collector/status.json に以下を追加（既存項目は保持）
leader: { host, since, active: true|false }
storage: { primary: "up|down", secondary: "used|idle", secondary_path }
sync: { pending: true|false, last: { at, items, bytes, ok } }
UI（Health）ではカード注釈として leader.host / storage.primary / sync.pending を短文表示。

### 28.3 リーダー選出（単一アクティブ収集）

ロックファイル: data/locks/collector.leader（NAS 上に作成）
フィールド: host, pid, started_at, heartbeat_ms
stale 判定: heartbeat_ms が T 秒以上更新されない場合にのみ奪取可（T 既定 30s）
遷移
収集 PC が通常リーダー → ロック保守・心拍更新
収集 PC 停止をメイン PC が検出 → 一時リーダー昇格（ロック奪取＋監査）
収集 PC 復帰 → 一時リーダーはロック検出で降格（監査）→ 収集停止
監査イベント
collector.leader.acquire / collector.leader.renew / collector.leader.release / collector.leader.stale_detected

### 28.4 ストレージ ルーティング（NAS ダウン時の退避）

書込先の選択: primary=NAS / secondary=local
判定 API：is_primary_available()（mkdir+tmp 書込の軽診断）
書込 API：write_atomic_csv(...) / append_jsonl(...)（内部で先を選択）
退避条件
is_primary_available=false → secondary へ切替（監査: collector.storage.switch）
storage.secondary=used を status.json に反映
ファイル規約（既存 22 章に準拠）
CSV：tmp→fsync→rename（置換型）
JSONL：append+flush+fsync（冪等追記）
ロック：data/locks/\*.lock（stale 自動解放）

### 28.5 同期（ローカル →NAS / 連続性保証）

起動トリガ
primary 復旧 → sync.pending=true に設定 → 同期スクリプト起動
手動/定時も可（ops/sync/sync_to_nas.ps1）
手順
検出：退避領域の対象（raw JSONL / latest CSV / audit）を列挙
検証：対象ごとに (exchange, topic, ts/ingest_id) の昇順で欠番確認
反映：NAS 側へ tmp→rename（CSV はハッシュ比較で冪等）
監査：collector.sync.complete|failed（items/bytes/elapsed）
掃除：ローカル退避分を削除（成功時のみ）
失敗時再試行：指数バックオフ＋最大回数（UI から再試行可）

### 28.6 監査・メトリクス

必須イベント
ストレージ切替：collector.storage.switch（primary→secondary / secondary→primary）
同期進捗：collector.sync.plan|start|chunk|complete|failed
リーダー遷移：28.3 のイベント群
メトリクス（audit.payload を集計）
sync.items, sync.bytes, sync.elapsed_ms, switch.count, leader.failover.count

### 28.7 実装コンポーネント（ファイルと責務）

common/
storage_router.py（新規）
役割: 書込先選択・軽診断・同期トリガ
I/F: is_primary_available(), append_jsonl, write_atomic_csv, plan_sync()
locks.py（既存）
役割: collector.leader ロック（stale 対応）
features/collector/core/
status.py（新規 / 先行実装対象）
役割: status.json の原子的更新、leader/storage/sync の状態反映
leader_lock.py（新規）
役割: ロック獲得・心拍更新・降格
worker.py（stub から開始）
役割: 各 exchange/topic ループ → status.update() まで
ops/
sync/sync_to_nas.ps1（新規）
役割: 退避 →NAS 同期（連続性検証 →tmp→rename→ 掃除）
diag/api_probe.ps1（既出案 / 収集 API 疎通）
それぞれ 2 行ヘッダ（# path / # desc） を付けること。

### 28.8 設定・パス（ENV と config）

ENV 優先（既存規約）：BTC_TS_DATA_DIR, BTC_TS_LOGS_DIR, BTC_TS_MODE
NAS/Secondary の解決
paths.primary_root() / paths.secondary_root() を paths.py に薄く定義
可能なら config/app.yaml に storage: { primary_root, secondary_root } を持たせる（UI は表示のみ）

### 28.9 フェイルシナリオと挙動（要点）

事象 期待挙動
NAS ダウン is_primary_available=false → secondary へ切替、監査記録、status.storage.secondary=used
NAS 復旧 同期計画 → 連続性検証 → 反映 → ローカル掃除、sync.complete 監査
収集 PC 停止 メイン PC が leader.lock stale 検知 → 昇格（監査）→ 収集継続
収集 PC 復帰 一時リーダー側がロック存在を検知 → 降格（監査）→ 収集停止
同期失敗 バックオフ再試行、失敗時 sync.failed、UI に “再試行” ボタン

### 28.10 API / I/F（抜粋）

# common/storage_router.py

class StorageRouter:
def is_primary_available(self) -> bool: ...
def append_jsonl(self, relpath: str, obj: dict) -> None: ...
def write_atomic_csv(self, relpath: str, rows: list[list[str]]) -> None: ...
def plan_sync(self) -> dict: ... # {targets, bytes, est_ms}

# features/collector/core/status.py

class StatusWriter:
def update(self, exchange, topic, \*, ok:bool|None, cause:str|None, retries:int|None, last_ok_ms:int|None, notes:str|None) -> None: ...
def flush(self) -> Path: ...

### 28.11 テストと検証（最小セット）

ユニット
common/test_storage_router.py：primary 切替・冪等追記・CSV 原子的置換
collector/test_status.py：更新・並び・ISO・原子的書込
スモーク（PowerShell）
ops/sync/smoke_sync.ps1：ダミー raw/CSV を secondary→primary へ同期、欠番検査
E2E 手順
Leader=収集 PC で収集開始（NAS 書込）
NAS を停止 → secondary 書込に切替されること
NAS 復旧 → 同期完了・ローカル掃除・連続性 OK を確認
収集 PC 停止 → メイン PC 昇格 → 復帰後の降格

### 28.12 セキュリティ・運用

NAS 権限：収集/メイン両 PC のサービスユーザに RW 付与（ロックと tmp 作成権限必須）
監査の機密：鍵・トークンはマスク（既存規約準拠）
容量管理：logs/archive/ と data/archive/raw/ の圧縮方針（22 章）を踏襲

### 28.13 段階導入（メイン PC から開始）

Phase A（今回実装範囲）
features/collector/core/status.py を導入（メイン PC 単体で出力可）
Health 連動の確認
Phase B
leader_lock.py（昇格/降格）＋ storage_router.py（切替のみ）
Phase C
sync_to_nas.ps1（検証 → 反映 → 掃除）＋ 欠番検査を有効化
Phase D
収集 PC へ展開 → 2 台＋ NAS 体制で本番運用開始
以後、Collector 本体（各ワーカ）や Audit ビューとは 既定 I/F（status.json / audit.jsonl） で自然接続する。

